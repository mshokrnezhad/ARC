{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file aims to retrieve information from a website and determine if we can integrate the data into our LLM to facilitate accurate responses to our queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing the Optional type from the typing module for type annotations.  \n",
    "from typing import Optional  \n",
    "\n",
    "# Importing the requests library to handle HTTP requests.  \n",
    "import requests  \n",
    "\n",
    "# Importing the re module for regular expression operations.  \n",
    "import re  \n",
    "\n",
    "# Importing RecursiveCharacterTextSplitter from langchain_text_splitters and aliasing it as TextSplitter.  \n",
    "# This is used for splitting text into manageable chunks recursively based on character limits.  \n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter as TextSplitter  \n",
    "\n",
    "# Importing the completion module from litellm.  \n",
    "# This is used for handling language model requests.\n",
    "from litellm import completion  \n",
    "\n",
    "# Importing load_dotenv from the dotenv package to load environment variables from a .env file.  \n",
    "from dotenv import load_dotenv  \n",
    "\n",
    "# Importing QdrantClient from qdrant_client to interact with the Qdrant vector database.  \n",
    "from qdrant_client import QdrantClient  \n",
    "\n",
    "# Importing VectorParams and Distance models from qdrant_client.models.  \n",
    "# These are used to define vector storage parameters and distance metrics.  \n",
    "from qdrant_client.models import VectorParams, Distance  \n",
    "\n",
    "# Importing SentenceTransformer from sentence_transformers to generate sentence embeddings.  \n",
    "from sentence_transformers import SentenceTransformer  \n",
    "\n",
    "# Importing Markdown and display from IPython.display to render Markdown in Jupyter notebooks.  \n",
    "from IPython.display import Markdown, display  \n",
    "\n",
    "# Importing DDGS from duckduckgo_search to perform web searches using DuckDuckGo's search engine.  \n",
    "from duckduckgo_search import DDGS  \n",
    "\n",
    "# Loading environment variables from the .env file into the environment.  \n",
    "# This allows the use of API keys and other sensitive information without hardcoding them.  \n",
    "load_dotenv()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_url(url_body: str) -> Optional[str]:\n",
    "    \"\"\"  \n",
    "    Fetches the content from a specified URL.  \n",
    "\n",
    "    Args:  \n",
    "        url_body (str): The suffix of the URL to fetch, appended to the base URL.  \n",
    "\n",
    "    Returns:  \n",
    "        Optional[str]: The decoded content of the response if successful; otherwise, None.  \n",
    "    \"\"\"  \n",
    "    \n",
    "    url_prefix: str = \"https://r.jina.ai/\"\n",
    "    url_full : str = url_prefix + url_body \n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url_full)\n",
    "        if response.status_code == 200:\n",
    "            return response.content.decode(\"utf-8\")\n",
    "        else:\n",
    "            print(f\"Error {response.status_code}: {response.text}\")\n",
    "            return None\n",
    "    except:\n",
    "        print(f\"Error: Fetching {url_body} failed.\")\n",
    "        return None\n",
    "    \n",
    "def clean_text(text):\n",
    "    \"\"\"  \n",
    "    Cleans the input text by removing unnecessary whitespace and line breaks.  \n",
    "\n",
    "    Args:  \n",
    "        text (str): The raw text to be cleaned.  \n",
    "\n",
    "    Returns:  \n",
    "        str: The cleaned and stripped text.  \n",
    "    \"\"\"  \n",
    "    \n",
    "    # Replace newline characters with a space  \n",
    "    text = text.replace(\"\\n\", \" \")  \n",
    "    # Replace carriage return characters with a space  \n",
    "    text = text.replace(\"\\r\", \" \")  \n",
    "    # Use a regular expression to replace multiple whitespace characters with a single space  \n",
    "    text = re.sub(r\"\\s+\", \" \", text)  \n",
    "    # Remove leading and trailing whitespace  \n",
    "    text = text.strip()  \n",
    "    \n",
    "    return text\n",
    "    \n",
    "def get_embeddings(texts, model_name=\"all-MiniLM-L6-v2\"):  \n",
    "    \"\"\"  \n",
    "    Generates embeddings for a list of texts using a specified SentenceTransformer model.  \n",
    "\n",
    "    Args:  \n",
    "        texts (list): A list of strings for which embeddings are to be generated.  \n",
    "        model_name (str, optional): The name of the pre-trained SentenceTransformer model to use. Defaults to \"all-MiniLM-L6-v2\".  \n",
    "\n",
    "    Returns:  \n",
    "        list: A list of embeddings corresponding to the input texts.  \n",
    "    \"\"\"  \n",
    "    \n",
    "    model = SentenceTransformer(model_name)   \n",
    "    embeddings = model.encode(texts)  \n",
    "    \n",
    "    return embeddings  \n",
    "\n",
    "def search(text: str, top_k: int, client, collection_name):\n",
    "    \"\"\"  \n",
    "    Searches for the top_k most similar vectors in the Qdrant collection based on the input text.  \n",
    "\n",
    "    Args:  \n",
    "        text (str): The query text to search for.  \n",
    "        top_k (int): The number of top results to retrieve.  \n",
    "        client (QdrantClient): An instance of QdrantClient to interact with the Qdrant service.  \n",
    "        collection_name (str): The name of the Qdrant collection to search within.  \n",
    "\n",
    "    Returns:  \n",
    "        list: A list of search results containing the most similar documents.  \n",
    "    \"\"\"  \n",
    "    \n",
    "    query_embedding = get_embeddings(text)\n",
    "    \n",
    "    result = client.search(\n",
    "        collection_name=collection_name,\n",
    "        query_vector= query_embedding,\n",
    "        query_filter=None,\n",
    "        limit=top_k\n",
    "    )\n",
    "    \n",
    "    return result\n",
    "\n",
    "def format_docs(docs):\n",
    "    \"\"\"  \n",
    "    Formats a list of documents by concatenating their 'content' payloads, separated by double newlines.  \n",
    "\n",
    "    Args:  \n",
    "        docs (list): A list of document objects retrieved from Qdrant.  \n",
    "\n",
    "    Returns:  \n",
    "        str: A single string containing the concatenated contents of all documents.  \n",
    "    \"\"\"  \n",
    "    \n",
    "    return \"\\n\\n\".join(doc.payload[\"content\"] for doc in docs)\n",
    "    \n",
    "def format_search_results(results):\n",
    "    \"\"\"  \n",
    "    Formats search results retrieved from DuckDuckGo by concatenating their 'body' fields, separated by double newlines.  \n",
    "\n",
    "    Args:  \n",
    "        results (list): A list of search result objects from DuckDuckGo.  \n",
    "\n",
    "    Returns:  \n",
    "        str: A single string containing the concatenated bodies of all search results.  \n",
    "    \"\"\"  \n",
    "    \n",
    "    return \"\\n\\n\".join(doc[\"body\"] for doc in results)\n",
    "\n",
    "def answer(question, client, collection_name):\n",
    "    \"\"\"  \n",
    "    Provides an answer to a given question using context retrieved from Qdrant. If the context is insufficient, it performs an online search.  \n",
    "\n",
    "    Args:  \n",
    "        question (str): The user's question to be answered.  \n",
    "        client (QdrantClient): An instance of QdrantClient to interact with the Qdrant service.  \n",
    "        collection_name (str): The name of the Qdrant collection to search within.  \n",
    "    \"\"\"  \n",
    "    \n",
    "    results = search(question, top_k=3, client=client, collection_name=collection_name)\n",
    "    context = format_docs(results)\n",
    "\n",
    "    system_prompt_1 = \"\"\"\n",
    "    Your task is to determine if a specific question can be answered using the provided context. \n",
    "    If so, return 1; otherwise, return 0. \n",
    "    Do not return anything other than 1 or 0. \n",
    "\n",
    "    Context:  {context}\n",
    "    \"\"\"\n",
    "\n",
    "    system_prompt_2 = \"\"\"\n",
    "    You are an expert for answering questions. Answer the question according only to the given context.\n",
    "    If question cannot be answered using the context, simply say I don't know. Do not make stuff up.\n",
    "    Your answer MUST be informative, concise, and action driven. Your response must be in Markdown.\n",
    "\n",
    "    Context: {context}\n",
    "    \"\"\"\n",
    "\n",
    "    user_prompt = \"\"\"\n",
    "    Question: {question}\n",
    "\n",
    "    Answer: \n",
    "    \"\"\"\n",
    "    \n",
    "    response = completion(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"content\": system_prompt_1.format(context=context),\"role\": \"system\"}, {\"content\": user_prompt.format(question=question),\"role\": \"user\"}],\n",
    "        max_tokens=500,\n",
    "        # format=\"json\"\n",
    "        )\n",
    "\n",
    "    has_answer = response.choices[0].message.content\n",
    "    \n",
    "\n",
    "    print(f\"Question: {question}\")\n",
    "    if has_answer == '1':\n",
    "        print(\"Context can answer the question\")\n",
    "        response = completion(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[{\"content\": system_prompt_2.format(context=context),\"role\": \"system\"}, {\"content\": user_prompt.format(question=question),\"role\": \"user\"}],\n",
    "            max_tokens=500\n",
    "        )\n",
    "        print(\"Answer:\")\n",
    "        display(Markdown(response.choices[0].message.content))\n",
    "    else:\n",
    "        print(\"Context is NOT relevant. Searching online...\")\n",
    "        results = DDGS().text(question, max_results=5)\n",
    "        context = format_search_results(results)\n",
    "        print(\"Found online sources. Generating the response...\")\n",
    "        response = completion(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[{\"content\": system_prompt_2.format(context=context),\"role\": \"system\"}, {\"content\": user_prompt.format(question=question),\"role\": \"user\"}],\n",
    "            max_tokens=500\n",
    "        )\n",
    "        print(\"Answer:\")\n",
    "        display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of chunks: 54\n"
     ]
    }
   ],
   "source": [
    "url_body: str = \"https://em360tech.com/tech-article/meta-gdpr-fine\"\n",
    "content : Optional[str] = fetch_url(url_body)\n",
    "\n",
    "texts = []\n",
    "metadatas = []\n",
    "texts.append(content)\n",
    "metadatas.append({\"url\": url_body})\n",
    "# Ensure that the lengths of texts and metadatas lists are equal.\n",
    "# This is crucial to maintain one-to-one correspondence between texts and their metadata.\n",
    "assert len(metadatas) == len(texts)\n",
    "\n",
    "# Initialize a text splitter using the RecursiveCharacterTextSplitter from LangChain.  \n",
    "# - model_name: Specifies the model to be used for tokenization (here, \"gpt-4\")  \n",
    "# - chunk_size: The maximum size of each text chunk (150 tokens)  \n",
    "# - chunk_overlap: The number of overlapping tokens between consecutive chunks (0 here, meaning no overlap)  \n",
    "text_splitter = TextSplitter.from_tiktoken_encoder(model_name=\"gpt-4\", chunk_size=150, chunk_overlap=0)\n",
    "text_chunks = text_splitter.split_text(content)\n",
    "print(f\"Total number of chunks: {len(text_chunks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mshokrnezhad/opt/anaconda3/envs/rag_llm/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "embeddings = get_embeddings(text_chunks)\n",
    "assert len(embeddings) == len(text_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountResult(count=54)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = QdrantClient(\"http://localhost:6333\")\n",
    "collection_name = \"agent_rag_index\"\n",
    "# The size of vectors need to be fixed based on the size of embedings. \n",
    "VECTOR_SIZE = 384\n",
    "\n",
    "client.delete_collection(collection_name=collection_name)\n",
    "client.create_collection(collection_name=collection_name, vectors_config=VectorParams(size=VECTOR_SIZE, distance=Distance.COSINE))\n",
    "\n",
    "ids = []\n",
    "payload = []\n",
    "\n",
    "for id, text in enumerate(text_chunks):\n",
    "    ids.append(id)\n",
    "    payload.append({\"url\": url_body, \"content\": text})\n",
    "\n",
    "client.upload_collection(\n",
    "    collection_name=collection_name,\n",
    "    vectors=embeddings,\n",
    "    payload=payload,\n",
    "    ids=ids,\n",
    "    batch_size=256\n",
    ")\n",
    "\n",
    "client.count(collection_name=collection_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Did Apple slam for GDPR violation?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Did Apple slam for GDPR violation?\n",
      "Context is NOT relevant. Searching online...\n",
      "Found online sources. Generating the response...\n",
      "Answer:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "I don't know."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "answer(question, client, collection_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
